### machine translation:
Basic Papers:

Attention models:
-  Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
   Bengio. 2015.[ Neural machine translation by jointly learning to align and translate](https://arxiv.org/pdf/1409.0473.pdf). ICLR.
-  Minh-Thang Luong, Hieu Pham, and Christopher D
   Manning. 2015.[ Effective approaches to attention-based neural machine translation](https://arxiv.org/pdf/1508.04025.pdf). EMNLP.
-  Ilya Sutskever, Oriol Vinyals, and Quoc
   V. Le. 2014.[ Sequence to sequence learning with neural networks](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf). NIPS.

SOTA Papers:

Datasets:
[WMT](https://github.com/tensorflow/nmt)(english to german)

Tutorials:
- raham Neubig. 2017. [Neural Machine Translation and Sequence-to-sequence Models:
A Tutorial](https://arxiv.org/pdf/1703.01619.pdf)

Code:
1. tensorflow [tutorial](https://github.com/tensorflow/nmt) by Luong.

1. [attetion models in Keras](https://machinelearningmastery.com/encoder-decoder-attention-sequence-to-sequence-prediction-keras/)

1. [Open-NMT](https://hackernoon.com/neural-machine-translation-using-open-nmt-for-training-a-translation-model-1129a3a2a2d3) for training a translation model.


1. Pytorch [tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html) in MT with attention:
Neural Machine Translator with Less than 50 lines of Code + Guide

1. [medium](https://medium.com/tensorflow/complete-code-examples-for-machine-translation-with-attention-image-captioning-text-generation-51663d07a63d)
